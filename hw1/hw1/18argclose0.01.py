import csv
import numpy as np
from math import sqrt

##### Debugging controller ========================================================================
_DISPLAY = 1

##### Number of iterations ========================================================================
ITERATION = 10000

##### Parameters ==================================================================================
# Null
w_list = [0 for x in range(163)]

# Error = 5.679551125230395
w_list = [0.37829825426194646, 0.061678009340105061, -0.13833740793295757, 0.18052008835401828, -0.26386376819460594, -0.0041195865918177534, 0.15791792762646611, -0.064219639176259405, -0.26582641583591854, 0.32263145092041073, -2.2952992693995555, 1.1889197889584491, 0.69906725722397789, 0.66995529864979875, 0.099654003105558761, -0.20604056256332759, -1.463665135772463, 1.211743400572036, 4.0563257060962981, 0.16099262683504417, -0.083741699298559999, 0.1908149051949177, -0.49059434651268236, 0.45809889231967821, -0.066252185131940602, -0.50738225386748859, 0.24409598037422112, 1.2281574965464244, -2.7445296338730372, 3.7186294094561125, -2.3669504320833301, 2.6355346532570274, 3.2344523439720012, -4.7634641575055809, 0.70462547947094512, 1.5420501757492222, 0.69828347216755537, 0.044990492701841592, 0.040503627698716148, 0.10341756137265584, -0.43316692614264773, -0.21639743041902681, -0.249664778278963, -0.026878086445228242, -0.2472123032421169, -0.29517899579912765, 0.048439362407558528, -0.019480699894837118, -0.037753296918832072, -0.42825236529382005, -0.24934335501690194, -0.1637936073545383, -0.09958876609042519, -0.44645161064566147, 0.027407389195403593, -0.0069172915128206686, -0.063277932778222759, 0.02280133794033987, 0.40197276159470829, 0.22801080726602546, 0.1761365936922841, 0.048940278418808679, 0.35459690543309391, 0.24922158350508125, 0.00050402147565381278, 0.01353320826967783, -0.010321282381994016, -0.018570784220349794, 0.00052088380903383611, -0.033126167348566118, -0.019434764383349807, -0.01325973317119533, 0.097440907071034838, 0.0083990922943579481, 0.0087345081658128324, -0.029640856320902745, 0.04202079806776602, -0.0076073174226911613, -0.038841656222714675, 0.038040593000379173, -0.001638111764137363, 0.034703250755942598, -0.035573620305014798, -0.012436546197772758, 0.21351146760625825, -0.23512952639973572, -0.020754227149070693, 0.49051016117183088, -0.5549389497744287, 0.023998563655443551, 0.95268538282911575, 0.03697440217590392, -0.0088665495250961467, -0.045679639631097606, 0.0093522914699714441, -0.055740251630779468, 0.048561347349292613, 0.017389542310075987, -0.040660237513648452, -0.077499892302944295, -0.0079887844367259458, 0.018163007892959413, 0.023509959759096519, -0.060483309436373069, -0.044083563068101086, 0.083161677108559359, -0.092665865972399147, 0.00070251363283244388, 0.050885907360307236, -0.28467041987963637, 0.33018031450268603, -0.0734770414239376, -0.053185785910666902, -0.0072295545572458294, 0.052839578576110963, -0.11332698001745099, 0.14128859631762797, 0.14591451921697701, 1.6935697883087395, -1.8305931195141598, 0.74219256297864167, -2.2466388006597198, -0.64229202454594947, 1.8511856180583843, 0.46731564891455363, -1.8355896584190969, -0.78124298951115767, -0.0004494944991289543, 0.0030273837083573227, -0.00032267329766393445, 0.0018737121207571654, 0.00045533384615850658, 0.0017869347102935219, -0.002529921878525155, 0.0011981032597246292, 0.00026842664219810578, -0.0020621042750845267, -0.001044909647598159, 0.00084623278113447127, -0.001893141950360232, 0.00038327799874532408, -2.709849472553784e-05, -2.7257016073566841e-05, -0.00239436438914314, 0.00044516896974081024, -0.17900605424294166, -0.12943797417838601, 0.23632618846264755, 0.076516601912457155, -0.097369286534845431, -0.074803993208826441, -0.031884931792391498, -0.0036239863747802889, -0.12390065294692724, -0.014700858870030363, 0.17447700755014828, -0.13551368366438507, -0.21606780224495264, -0.05122135393493786, 0.33482594689486522, -0.0037522138623886235, -0.31562017167813083, 0.19684306839078752]

# Gradients
w_grad = [0 for x in range(163)]

# Root-sum-square of gradients over time
w_grad_rms = [0 for x in range(163)]

# Regulation
lamda = 0.01

# Learning rates
w_lr = [0.001] + [0.001 for x in range(162)]
##### Variables ===================================================================================
data = [[[] for arg in range(18)] for month in range(12)]
x_list = []
y_list = []
err = 0
err_ss = 0
min_err = 0
w_best = []

##### Read in the training file ===================================================================
with open('train.csv', encoding='mac_roman', newline = '') as csv_file:
	# Read the file and delete the first line
	csv_obj = csv.reader(csv_file)
	all_data = list(csv_obj)
	del all_data[0]
	# Setting data (18 X 5760 array)
	for month in range(12):
		for day in range(20):
			for hour in range(24):
				for arg in range(18):
					if(all_data[18*(20*month+day)+arg][hour+3] == 'NR'):
						data[month][arg].append(0)
					else:
						data[month][arg].append(float(all_data[18*(20*month+day)+arg][hour+3]))
for month in range(12):
	for index in range(471):
		tmplist = [1]
		for arg in range(18):
			tmplist += [data[month][arg][index+x] for x in range(9)]
		x_list.append(tmplist)
		y_list.append([data[month][9][index+9]])

x = np.array(x_list)
y = np.array(y_list)
xt = x.transpose()
w = np.dot(np.dot(np.linalg.inv(np.dot(xt, x)), xt), y)

w_list = [w[x][0] for x in range(163)]
#print(w_list)
##### Train with regulation =======================================================================
for it in range(ITERATION):
	if(_DISPLAY == 1 and it % 10 == 0):
		print("Iteration : ", it, " / ", ITERATION)
	w_grad = [0 for x in range(163)]
	for row in range(len(x_list)):
		xsum = 0
		for x in range(163):
			xsum += w_list[x] * x_list[row][x]
		w_grad = [w_grad[x] - 2 * x_list[row][x] * (y_list[row][0] - xsum) + 2 * lamda * w_list[x] for x in range(163)]
	w_grad_rms = [sqrt(pow(w_grad_rms[x], 2) + pow(w_grad[x], 2)).real for x in range(163)]
	w_list = [w_list[x] - w_lr[x] * w_grad[x] / w_grad_rms[x] for x in range(163)]

	if(_DISPLAY == 1 and it % 10 == 0):
		err_ss = 0
		for row in range(len(x_list)):
			xsum = 0
			for x in range(163):
				xsum += w_list[x] * x_list[row][x]
			err_ss += pow(y_list[row][0] - xsum, 2)
		err = sqrt(err_ss/len(x_list)).real
		if(it == 0 or err < min_err):
			min_err = err
			w_best = w_list
			print("w : ", w_list)
		print("Error : ", err)
'''
err_ss = 0
for row in range(len(x_list)):
	xsum = 0
	for x in range(163):
		xsum += w_list[x] * x_list[row][x]
	err_ss += pow(y_list[row][0] - xsum, 2)
err = sqrt(err_ss/row).real
print('Error : ', err)
'''






















