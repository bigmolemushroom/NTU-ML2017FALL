import csv
import numpy as np
from math import sqrt

##### Debugging controller ========================================================================
_DISPLAY = 1

##### Number of iterations ========================================================================
ITERATION = 10000

##### Parameters ==================================================================================
# Null
w_list = [0 for x in range(163)]

# Error : 5.679617816012892
w_list = [0.37262750780748849, 0.061611219808307721, -0.13824646961154222, 0.18040293270449576, -0.26369541551808751, -0.0041336472640272981, 0.15780829661791448, -0.06418127266824418, -0.26565594395113795, 0.32242731511956801, -2.2784487930219144, 1.1788222555643058, 0.69289576592240532, 0.66403113830023186, 0.098747620175354711, -0.20420345541960291, -1.4517321272189143, 1.2014717754310877, 4.0337343852351886, 0.15523590769175533, -0.080616805720118825, 0.18393873348738174, -0.47531606878735755, 0.44345478925995718, -0.063703868302184108, -0.49165851104252495, 0.23530376308330747, 1.2033707905322408, -2.7475714087653902, 3.7281924272762179, -2.3664227259042168, 2.6368047971927262, 3.2405449309619132, -4.7780276679306146, 0.68151449892471094, 1.5304016046542686, 0.67443906101056861, 0.044519901119532759, 0.040199878604790101, 0.1021013904935351, -0.43094011041095664, -0.21523754217567817, -0.24798738075682406, -0.026535123367275041, -0.24528025697885414, -0.29287316943446895, 0.04843998630503904, -0.019287829719191414, -0.03779553609336396, -0.42777132524655503, -0.24899146722994661, -0.16354864000331051, -0.099396825369834338, -0.44578347525055995, 0.027299768660311783, -0.0068767827014846687, -0.063076311400505278, 0.022574621027010099, 0.40129347061053416, 0.22769119521455602, 0.17594777227115313, 0.048952654487158992, 0.35426105196246988, 0.24891941011534671, 0.00051157827484000845, 0.013352219830895006, -0.010193051813011222, -0.018461886885206152, 0.00035859961932091462, -0.033024196204119415, -0.019196541340567592, -0.013157277642998922, 0.097259556492916732, 0.0082353494999121424, 0.0084966806375212141, -0.029497075489898762, 0.041909905096170132, -0.0074733036152310629, -0.038694109947371727, 0.037990417877711999, -0.0013756799716020431, 0.034592069192194902, -0.035227006007161825, -0.012230915230987774, 0.21331750801912377, -0.23497842555963119, -0.020625889659085472, 0.49015955466883188, -0.55449962772682948, 0.023935324406840833, 0.95205578546370329, 0.035879948004845418, -0.008492135316987082, -0.044435849745188098, 0.0082261246144919465, -0.053880345032454202, 0.047208566741640277, 0.016390915024572874, -0.040264944453656165, -0.076185053053102747, -0.0079653894095110186, 0.01814275947681257, 0.023477595523219656, -0.060450487742492176, -0.044027634894557965, 0.0831643921238744, -0.092620212423902726, 0.00072244512379361343, 0.050852538393919185, -0.28309572181661713, 0.3286110888008022, -0.07295555120577106, -0.052779947141148024, -0.0070008641340275329, 0.052634121902543581, -0.11265633107700823, 0.14064813704054133, 0.14516444105761148, 1.6808461706616737, -1.8172149736298726, 0.73614595752010181, -2.2310157196831382, -0.63714956609501028, 1.837514410054615, 0.4634794975853897, -1.8221896150504493, -0.77501167830653594, -0.00043311414574054893, 0.0029590296711036169, -0.00025427184193290667, 0.0018240196386842945, 0.00044476719149174915, 0.0017423910197833098, -0.0025199939930675877, 0.0011258968442185797, 0.00025791036455834827, -0.0020503847608083188, -0.0010119193156117123, 0.0008120218739963214, -0.001855823860613136, 0.00039691346885987225, -4.9458989890492403e-07, -1.6799927299510591e-05, -0.0023412739613666309, 0.00045378906507028457, -0.17784003084798158, -0.12861681000070427, 0.23473812041747805, 0.075960256686814775, -0.096735096486694441, -0.074252378571428759, -0.031580875609022015, -0.0035095764380873463, -0.12303261563486456, -0.01450506716473518, 0.17295054599241536, -0.13431627070136173, -0.21417928863628727, -0.050771266632204555, 0.33199626797804627, -0.0035790100605418073, -0.31282001614335436, 0.19514379156995526]

# Gradients
w_grad = [0 for x in range(163)]

# Root-sum-square of gradients over time
w_grad_rms = [0 for x in range(163)]

# Regulation
lamda = 0.1

# Learning rates
w_lr = [0.001] + [0.001 for x in range(162)]
##### Variables ===================================================================================
data = [[[] for arg in range(18)] for month in range(12)]
x_list = []
y_list = []
err = 0
err_ss = 0
min_err = 0
w_best = []

##### Read in the training file ===================================================================
with open('train.csv', encoding='mac_roman', newline = '') as csv_file:
	# Read the file and delete the first line
	csv_obj = csv.reader(csv_file)
	all_data = list(csv_obj)
	del all_data[0]
	# Setting data (18 X 5760 array)
	for month in range(12):
		for day in range(20):
			for hour in range(24):
				for arg in range(18):
					if(all_data[18*(20*month+day)+arg][hour+3] == 'NR'):
						data[month][arg].append(0)
					else:
						data[month][arg].append(float(all_data[18*(20*month+day)+arg][hour+3]))
for month in range(12):
	for index in range(471):
		tmplist = [1]
		for arg in range(18):
			tmplist += [data[month][arg][index+x] for x in range(9)]
		x_list.append(tmplist)
		y_list.append([data[month][9][index+9]])

x = np.array(x_list)
y = np.array(y_list)
xt = x.transpose()
w = np.dot(np.dot(np.linalg.inv(np.dot(xt, x)), xt), y)

w_list = [w[x][0] for x in range(163)]
#print(w_list)
##### Train with regulation =======================================================================
for it in range(ITERATION):
	if(_DISPLAY == 1 and it % 10 == 0):
		print("Iteration : ", it, " / ", ITERATION)
	w_grad = [0 for x in range(163)]
	for row in range(len(x_list)):
		xsum = 0
		for x in range(163):
			xsum += w_list[x] * x_list[row][x]
		w_grad = [w_grad[x] - 2 * x_list[row][x] * (y_list[row][0] - xsum) + 2 * lamda * w_list[x] for x in range(163)]
	w_grad_rms = [sqrt(pow(w_grad_rms[x], 2) + pow(w_grad[x], 2)).real for x in range(163)]
	w_list = [w_list[x] - w_lr[x] * w_grad[x] / w_grad_rms[x] for x in range(163)]

	if(_DISPLAY == 1 and it % 10 == 0):
		err_ss = 0
		for row in range(len(x_list)):
			xsum = 0
			for x in range(163):
				xsum += w_list[x] * x_list[row][x]
			err_ss += pow(y_list[row][0] - xsum, 2)
		err = sqrt(err_ss/len(x_list)).real
		if(it == 0 or err < min_err):
			min_err = err
			w_best = w_list
			print("w : ", w_list)
		print("Error : ", err)
'''
err_ss = 0
for row in range(len(x_list)):
	xsum = 0
	for x in range(163):
		xsum += w_list[x] * x_list[row][x]
	err_ss += pow(y_list[row][0] - xsum, 2)
err = sqrt(err_ss/row).real
print('Error : ', err)
'''






















