import csv
import numpy as np
from math import sqrt

##### Debugging controller ========================================================================
_DISPLAY = 1

##### Number of iterations ========================================================================
ITERATION = 10000

##### Parameters ==================================================================================
# Null
w_list = [0 for x in range(163)]

# Error : 5.679524951460221
w_list = [0.38018666990753291, 0.061779320371881948, -0.13841179203385545, 0.18051875486472199, -0.26393558300013081, -0.0041010721304162306, 0.15794773349628902, -0.064241146581392128, -0.26591344713985615, 0.32261203859845311, -2.3017426869485962, 1.1923475892402129, 0.70111952159367197, 0.67191618852379531, 0.099988525137437292, -0.20659299228723685, -1.4678183239461442, 1.2152089034342395, 4.0678053727656662, 0.16336543189531358, -0.08489526823074213, 0.19338899591085021, -0.49649261536321976, 0.46311467214669166, -0.067435719753586434, -0.5138225465020102, 0.24670770158371683, 1.2421894872788257, -2.7994897380484729, 3.7802788522902486, -2.4185406764653838, 2.6901403671663524, 3.2938317248163345, -4.8279811742504908, 0.72597084910517151, 1.5827608975475802, 0.72005553393513932, 0.045324892210894419, 0.040624691776383511, 0.10445095858750215, -0.43404841020278673, -0.21664582041090669, -0.25005352803850373, -0.027053441605513553, -0.24810577257476235, -0.29641968046680739, 0.048604508766586306, -0.019839648796144516, -0.037714752324040056, -0.42851459071371195, -0.24954567745440809, -0.16376958864146882, -0.099870882572350869, -0.44690996376816272, 0.027558678761481929, -0.006798567720188246, -0.063589692321795205, 0.023012436906048938, 0.40208552420041144, 0.22809871369033707, 0.1763207152682307, 0.048766288961880566, 0.35447038416211712, 0.24932665181098154, 0.00028458837934428283, 0.014049051754412741, -0.01062043954137558, -0.018766315108763778, 0.0011273494326877588, -0.033334764076631482, -0.019872711898203971, -0.013504023403741135, 0.097831221047183059, 0.0085213285478360282, 0.0089801348403223281, -0.030158645463590598, 0.04249723319907945, -0.0078747114893769031, -0.039020696784644836, 0.038540440341697686, -0.0022054071004226432, 0.034906456705758454, -0.035834144805891414, -0.012653267888393688, 0.21407921537225152, -0.23526786673131017, -0.021044810873658313, 0.49099910195742463, -0.55531312839415936, 0.024121374442815278, 0.95276584912668127, 0.037250905800887558, -0.0089477943132457126, -0.045992652844454696, 0.0096953096893171858, -0.056125405462242942, 0.048888612289612519, 0.017480966923727245, -0.040566014073082676, -0.077706524716074185, -0.0080716479415609693, 0.018292839850809251, 0.023669179766705732, -0.060554942088435447, -0.044242751305500587, 0.083130720826942686, -0.092779324951227898, 0.00064673806818503076, 0.051056809302719608, -0.28521970145831937, 0.33084153265024441, -0.073883409282929038, -0.053699492864207186, -0.0076291799374266301, 0.053132653750676585, -0.11365263307449489, 0.14155833120436973, 0.14637771603331204, 1.6980766717238194, -1.835373117963909, 0.74417407017958714, -2.2524984931430008, -0.64394185302295637, 1.8560837643921417, 0.46852345470382761, -1.8404444204334305, -0.78329216758459641, -0.00045348359726606947, 0.0030231854073734122, -0.00031815050182704911, 0.0018721730902673359, 0.00045610182642755631, 0.0017902511139814919, -0.0025266867099005114, 0.0012015279535224379, 0.00027318264771517323, -0.0020570671044097626, -0.0010479671572686741, 0.00085025623374724495, -0.001894549447532462, 0.00038425190741826575, -2.4566171505838094e-05, -2.5447665162193519e-05, -0.0023972829588352175, 0.00044328212784720339, -0.17937793740025956, -0.1296826985554807, 0.23700871348826988, 0.076915389873931928, -0.097631951942258224, -0.074972052021497218, -0.032072739641124524, -0.003791600149880313, -0.12429184651210602, -0.015075504155716181, 0.17506889912855195, -0.13598741561462394, -0.21669860439798699, -0.051445106410616434, 0.3357119776821218, -0.0041430643709093303, -0.31671601006597339, 0.19744872510792214]

# Gradients
w_grad = [0 for x in range(163)]

# Root-sum-square of gradients over time
w_grad_rms = [0 for x in range(163)]

# Regulation
lamda = 0.0001

# Learning rates
w_lr = [0.05] + [0.01 for x in range(162)]
##### Variables ===================================================================================
data = [[[] for arg in range(18)] for month in range(12)]
x_list = []
y_list = []
err = 0
err_ss = 0
min_err = 0
w_best = []

##### Read in the training file ===================================================================
with open('train.csv', encoding='mac_roman', newline = '') as csv_file:
	# Read the file and delete the first line
	csv_obj = csv.reader(csv_file)
	all_data = list(csv_obj)
	del all_data[0]
	# Setting data (18 X 5760 array)
	for month in range(12):
		for day in range(20):
			for hour in range(24):
				for arg in range(18):
					if(all_data[18*(20*month+day)+arg][hour+3] == 'NR'):
						data[month][arg].append(0)
					else:
						data[month][arg].append(float(all_data[18*(20*month+day)+arg][hour+3]))
for month in range(12):
	for index in range(471):
		tmplist = [1]
		for arg in range(18):
			tmplist += [data[month][arg][index+x] for x in range(9)]
		x_list.append(tmplist)
		y_list.append([data[month][9][index+9]])

x = np.array(x_list)
y = np.array(y_list)
xt = x.transpose()
w = np.dot(np.dot(np.linalg.inv(np.dot(xt, x)), xt), y)

w_list = [w[x][0] for x in range(163)]
#print(w_list)
##### Train with regulation =======================================================================
for it in range(ITERATION):
	if(_DISPLAY == 1 and it % 10 == 0):
		print("Iteration : ", it, " / ", ITERATION)
	w_grad = [0 for x in range(163)]
	for row in range(len(x_list)):
		xsum = 0
		for x in range(163):
			xsum += w_list[x] * x_list[row][x]
		w_grad = [w_grad[x] - 2 * x_list[row][x] * (y_list[row][0] - xsum) + 2 * lamda * w_list[x] for x in range(163)]
	w_grad_rms = [sqrt(pow(w_grad_rms[x], 2) + pow(w_grad[x], 2)).real for x in range(163)]
	w_list = [w_list[x] - w_lr[x] * w_grad[x] / w_grad_rms[x] for x in range(163)]

	if(_DISPLAY == 1 and it % 10 == 0):
		err_ss = 0
		for row in range(len(x_list)):
			xsum = 0
			for x in range(163):
				xsum += w_list[x] * x_list[row][x]
			err_ss += pow(y_list[row][0] - xsum, 2)
		err = sqrt(err_ss/len(x_list)).real
		if(it == 0 or err < min_err):
			min_err = err
			w_best = w_list
			print("w : ", w_list)
		print("Error : ", err)
'''
err_ss = 0
for row in range(len(x_list)):
	xsum = 0
	for x in range(163):
		xsum += w_list[x] * x_list[row][x]
	err_ss += pow(y_list[row][0] - xsum, 2)
err = sqrt(err_ss/row).real
print('Error : ', err)
'''






















